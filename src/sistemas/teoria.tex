\documentclass[../ecuaciones_diferenciales.tex]{subfiles}

\begin{document}

En este capítulo estudiaremos métodos para resolver sistemas de ecuaciones
lineales. Definimos primero el caso general, aunque por ahora nos restrigiremos
al caso lineal.
Sean \(x_1, x_2, \dots, x_n \in C^1(\alpha, \omega)\) variables, que son
funciones de clase \(C^1\) sobre un intervalo de \(\R\).

\begin{definition}
	Un sistema de ecuaciones diferenciales de primer orden es un sistema de la
	forma:
	\[\eqsys{
			x'_1 &= f_1(t, x_1, x_2, \dots, x_n) \\
			x'_2 &= f_2(t, x_1, x_2, \dots, x_n) \\
			&\vdots \\
			x'_n &= f_n(t, x_1, x_2, \dots, x_n)}\]
\end{definition}

\begin{definition}
	\label{def:siseq1ord}
	Un sistema de ecuaciones diferenciales lineales de primer orden es de la
	forma:
	\[\eqsys{
			x'_1 &= a_{11}(t)x_1 + a_{12}(t)x_2 + \dots + a_{1n}(t)x_n + b_1(t) \\
			x'_2 &= a_{21}(t)x_1 + a_{22}(t)x_2 + \dots + a_{2n}(t)x_n + b_2(t) \\
			&\vdots \\
			x'_n &= a_{n1}(t)x_1 + a_{n2}(t)x_2 + \dots + a_{nn}(t)x_n + b_n(t)}\]
	donde \(a_{ij}\), \(b_i\) son funciones continuas en un intervalo
	\((\alpha, \omega) \subset \R\) para \(i,j = 1, \dots, n\).
\end{definition}

Podemos expresar un sistema de este tipo en forma matricial como
\[\vec{x}' = A\vec{x} + \vec{b}\]
donde \(\vec{x} : \R \to \R^n\), \(A \in \mathcal{M}_{n \times n}(\R)\) y
\(\vec{b} : \R \to \R^n\) son tales que
\[\vec{x} = \vec{x}(t) = \mat{x_1(t) \\ x_2(t) \\ \vdots \\ x_n(t)}, \quad
	A = A(t) = \mat{a_{ij}(t)}^n_{i,j = 1},\quad
	\vec{b} = \vec{b}(t) = \mat{b_1(t) \\ b_2(t) \\ \vdots \\ b_n(t)} \]
y las derivadas e integrales se aplican \textquote{coordenada a coordenada}, 
v.g.
\[\vec{x}' = \mat{x'_1(t) \\ x'_2(t) \\ \vdots \\ x'_n(t)}
	= \mat{x'_1 \\ x'_2 \\ \vdots \\ x'_n}, \quad
	A' = \mat{a'_{ij}(t)}^n_{i,j = 1}, \quad
	\int A(s) \dif s = \mat{\int a_{ij} \dif s}^n_{i,j = 1}.\]

\section{Problema del valor inicial}

El problema del valor inicial para un sistema en forma matricial se define de
forma análoga a la forma escalar, cambiando escalares por vectores donde sea
necesario.

\begin{definition}
	Sean \(t_0 \in (\alpha, \omega)\) y
	\(\vec{x}^0 = \mat{x^0_1, x^0_2, \dots, x^0_n} \in \R^n\), 
	el problema del valor inicial asociado a~\ref{def:siseq1ord} es de la forma
	\[\eqsys{
		\vec{x}' = A(t)\vec{x} + \vec{b}(t) \\
		\vec{x}(t_0) = \vec{x}^0}\]
\end{definition}

Probaremos que el problema de valor inicial tiene solución única en todo el
dominio \((\alpha, \omega)\), para ello representaremos el problema del valor
inicial como una ecuación integral, es una estrategia general en análisis
intentar representar las funciones como integrales debido a su buen
comportamiento. Ser solución del problema del valor inicial equivale a
\[\vec{x}(t) = \vec{x}^0 + \int_{t_0}^t A(s)\vec{x}(s) + \vec{b}(s) \dif s.\]

\begin{notation}
	Denotaremos por \(C(I, \R^n)\) el espacio vectorial de las funciones 
	continuas del intervalo \(I \subset \R\) en \(\R^n\).
\end{notation}

A raíz del sistema~\ref{def:siseq1ord} definimos el siguiente operador.

\begin{definition}[Operador \(T\)]
	Definimos el operador:
	\begin{align*}
		T : C((\alpha, \omega), \R^n) & \to C((\alpha, \omega), \R^n) \\
		\phi                          & \mapsto 
			\vec{x}^0 + \int_{t_0}^t A(s) \phi(s) + \vec{b}(s) \dif s
	\end{align*}
\end{definition}

Evidentemente \(\vec{x}\) es solución del sistema si y solo si 
\(T(\vec{x}) = \vec{x}\), por lo que si demostramos que el operador \(T\) 
tiene un único punto fijo habremos terminado.

\begin{remark}
	En realidad, \(T(\vec{\phi}) \in C^1((\alpha, \omega), \R^n)\) para
	\(\vec{\phi} \in C((\alpha, \omega), \R^n)\); 
	hemos definido \(T\) con espacio de
	llegada \(C((\alpha, \omega), \R^n)\), del que 
	\(C^1((\alpha, \omega), \R^n)\) es subespacio vectorial, 
	para poder aplicar el teorema del punto fijo de
	Banach, que requiere espacios de salida y llegada iguales.
\end{remark}

Consideraremos \(\alpha < \tilde{\alpha} < t_0 < \tilde{\omega} < \omega\), y
demostraremos que \(T\) tiene un único punto fijo como operador en
\(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\). Haciendo tender \(\tilde{\alpha}
\to \alpha,\ \tilde{\omega} \to \omega\), tendremos el resultado deseado.

Antes de seguir necesitamos recordar algunos resultados de cálculo diferencial,
aunque demostraremos solo los que introduzcan algún concepto nuevo con respecto 
a lo visto en esa materia.

\begin{definition}[Espacio métrico completo]
	Un espacio métrico es completo si toda sucesión de Cauchy es convergente.
\end{definition}

\begin{theorem}[Punto fijo de Banach]
	Sean \((X, d)\) un espacio métrico completo y \(T : X \to X\) una aplicación
	contractiva (es decir, Lipschitz con constante \(\alpha \in [0, 1)\)), 
	entonces \(T\) tiene un único punto fijo.
\end{theorem}

Este teorema tiene una consecuencia que nos será muy útil.

\begin{corollary} \label{cor:banach_fixed_point}
	Sea \((X, d)\) un espacio métrico completo y \(T : X \to X\) tal que
	\(T^m = T \circ \overset{m}{\cdots} \circ T\) es contractiva para algún
	\(m \in \N\), entonces \(T\) tiene un único punto fijo.
\end{corollary}

Definimos ahora la distancia respecto a la cual el espacio 
\(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) es completo.

\begin{definition}[Distancia entre funciones]
	Definimos en \(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) la distancia
	\[d(\vec{f}, \vec{g}) := \norm{\vec{f} - \vec{g}}_{\infty} 
		= \max_{t \in [\tilde{\alpha}, \tilde{\omega}]} 
			\norm{\vec{f}(t) - \vec{g}(t)}_2,\]
	donde \(\norm{\cdot}_2\) es la norma euclídea en \(\R^n\). 
\end{definition}

\begin{proposition}
	\((C([\tilde{\alpha}, \tilde{\omega}], \R^n), d)\) 
	es un espacio métrico completo.
\end{proposition}

\begin{proof}
	Para comprobar que \(d\) es una distancia, basta ver que
	\[\norm{\vec{f}}_\infty 
		:= \max_{t \in [\tilde{\alpha}, \tilde{\omega}]} 
			\norm{\vec{f}(t)}_2\]
	es una norma (basta usar que \(\norm{\cdot}_2\) es una norma y las
	propiedades del máximo).

	Para ver que es completo, consideramos \((\vec{f}_n)_n\) una sucesión de 
	Cauchy arbitraria en \(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\), 
	lo que quiere decir que para cualquier \(\epsilon > 0\) existe un 
	natural \(N_0\) tal que si \(n, m \geq N_0\) entonces
	\[\norm{\vec{f}_n - \vec{f}_m}_\infty 
		= d(\vec{f}_n, \vec{f}_m) < \epsilon.\] 
	Fijamos ahora \(t_0 \in [\tilde{\alpha}, \tilde{\omega}]\) y consideramos la 
	sucesión \((\vec{f}_n(t_0))_n \subset \R^n\), que es de Cauchy porque
	\[\norm{\vec{f}_n(t_0) - \vec{f}_m(t_0)}_2 
		\leq \norm{\vec{f}_n - \vec{f}_m}_\infty.\] 
	Usamos la completitud de \(\R^n\) para obtener un límite 
	\(\vec{f}(t_0) = \lim_n \vec{f}_n(t_0)\), lo
	que nos proporciona una función 
	\(\vec{f}: [\tilde{\alpha}, \tilde{\omega}] \to \R^n\) que será la candidata
	a límite. Lo único que nos queda por ver es que
	\(\vec{f} \in C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) y 
	\(\vec{f}_n \overset{d}{\to} \vec{f}\).

	Dado \(\epsilon > 0\), existe \(N_0 \in \N\) tal que si \(n, m \geq N_0\)
	entonces 
	\[\norm{\vec{f}_n(t) - \vec{f}_m(t)}_2 
		\leq \norm{\vec{f}_n - \vec{f}_m}_\infty < \epsilon\]
	para todo \(t \in [\tilde{\alpha}, \tilde{\omega}]\), y haciendo tender
	\(m \to \infty\), se tiene que
	\(\norm{\vec{f}_n(t) - \vec{f}(t)}_2 \leq \epsilon\) para todo
	\(t \in [\tilde{\alpha}, \tilde{\omega}]\), de donde
	\(\norm{\vec{f}_n - \vec{f}}_\infty \leq \epsilon\).

	Queda así demostrado que \(\vec{f}_n \to \vec{f}\) uniformemente; 
	por un lado, esto implica que \(\vec{f}_n \overset{d}{\to} \vec{f}\) y, 
	por otro, que \(\vec{f}\) es continua (por serlo las \(\vec{f}_n\)).
\end{proof}

\begin{remark}
	La estrategia de esta demostración para probar la completitud es frecuente en
	espacios de funciones.
\end{remark}

Para demostrar el teorema de existencia y unicidad de la solución, únicamente
nos queda por ver que existe un \(m \in \N\) tal que \(T^m\) es contractiva.

\begin{corollary} \label{prop:operator_int_ineq}
	Sean \(\vec{f}, \vec{g} \in C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) y 
	\(t \in [\tilde{\alpha}, \tilde{\omega}]\). Se da la desigualdad:
	\[\norm{T(\vec{f})(t) - T(\vec{g})(t)}_2 
		= \norm{\int_{t_0}^t A(s)(\vec{f}(s) - \vec{g}(s)) \dif s}_2 
		\leq \int_{t_0}^t \norm{A(s)(\vec{f}(s) - \vec{g}(s))}_2 \dif s.\]
\end{corollary}

\begin{definition}[Norma operador]
	Dada una matriz \(M \in \mathcal{M}_{n \times n}\), definimos su norma 
	como operador:
	\[\norm{M}_{op} := \max_{\norm{\vec{x}}_2 \leq 1} \norm{M \vec{x}}_2 
		= \sup_{\vec{x} \neq \vec{0}}
			\frac{\norm{M \vec{x}}_2}{\norm{\vec{x}}_2}\]
\end{definition}

\begin{proposition} \label{prop:norm_op_eq}
	Para toda matriz \(M \in \mathcal{M}_{n \times n}\) y todo 
	\(\vec{x} \in \R^n\) se cumple 
	\[\norm{M \vec{x}}_2 \leq \norm{M}_{op} \norm{\vec{x}}_2.\]
\end{proposition}

\begin{definition}[Norma infinito]
	Si \(A : X \subset \R \to \mathcal{M}_{n \times n}\), definimos su norma 
	infinito como
	\[\norm{A}_\infty := \max_{x \in X} \norm{A(x)}_{op}\]
\end{definition}

Con esto del corolario~\ref{prop:operator_int_ineq} y la 
proposición~\ref{prop:norm_op_eq} se deduce que
\begin{align*}
	\norm{T(\vec{f})(t) - T(\vec{g})(t)}_2 
	&\leq \int_{t_0}^t \norm{A(s)}_{op} \norm{\vec{f}(s) - \vec{g}(s)}_2 \dif s
	\\
	&\leq \int_{t_0}^t \norm{A}_\infty \norm{\vec{f} - \vec{g}}_\infty \dif s \\
	&= \norm{A}_\infty\norm{\vec{f} - \vec{g}}_\infty(t - t_0).
\end{align*}
Como \(t, t_0 \in [\tilde{\alpha}, \tilde{\omega}]\), se tiene
\(\norm{T(\vec{f})(t) - T(\vec{g})(t)}_2 
	\leq \norm{A}_\infty\norm{\vec{f} - \vec{g}}_2 
		(\tilde{\omega} - \tilde{\alpha})\) y, tomando supremos, 
\[\norm{T(\vec{f}) - T(\vec{g})}_\infty 
	\leq \norm{A}_\infty(\tilde{\omega} 
	- \tilde{\alpha}) \cdot \norm{\vec{f} - \vec{g}}_\infty,\]
es decir, \(T\) es Lipschitz de
constante \(\norm{A}_\infty(\tilde{\omega}-\tilde{\alpha})\). Habríamos
acabado en el caso de que 
\(\norm{A}_\infty(\tilde{\omega}-\tilde{\alpha}) < 1\); si no, aún nos queda un
poco de trabajo.

\begin{lemma} \label{lem:desinfty}
	Para todo \(m \in \N\) se da la desigualdad:
	\[\norm{T^m(\vec{f})(t) - T^m(\vec{g})(t)}_2 
		\leq \frac{\norm{A}^m_\infty}{m!} \abs{t - t_0}^m 
			\norm{\vec{f} - \vec{g}}_\infty.\]
\end{lemma}

\begin{proof}
	El caso base \(m = 1\) está probado; suponemos el resultado cierto para
	\(m\) y para \(m + 1\) se tiene:
	\begin{align*}
		\norm{T^{m + 1}(\vec{f})(t) - T^{m + 1}(\vec{g})(t)}_2
		 & = \norm{T(T^m(\vec{f}))(t) - T(T^m (\vec{g}))(t)}_2 \\
		 & \leq \int_{t_0}^t \norm{A(s)}_{op}
		\norm{T^m(\vec{f})(s) - T^m (\vec{g})(s)}_2 \dif s,
	\end{align*}
	por la hipótesis de inducción:
	\begin{align*}
		\int_{t_0}^t \norm{A(s)}_{op}
		\norm{T^m(\vec{f})(s) - T^m(\vec{g})(s)}_2 \dif s
		 & \leq \int_{t_0}^t \norm{A}_\infty
		\frac{\norm{A}^m_\infty}{m!} \abs{s - t_0}^m
		\norm{\vec{f} - \vec{g}}_\infty \dif s \\
		 & \leq \frac{\norm{A}^{m + 1}_\infty}{m!} 
		 \norm{\vec{f} - \vec{g}}_\infty \int_{t_0}^t \abs{s - t_0}^m \dif s,
	\end{align*}
	ahora sin más que integrar:
	\begin{align*}
		\norm{T^{m + 1}(\vec{f})(t) - T^{m + 1}(\vec{g})(t)}_2
		&\leq \frac{\norm{A}^{m + 1}_\infty}{m!} 
		 	\norm{\vec{f} - \vec{g}}_\infty \frac{\abs{t - t_0}^{m + 1}}{m + 1} 
		\\
		&= \frac{\norm{A}^{m + 1}_\infty}{(m + 1)!} \abs{t - t_0}^{m + 1}
		\norm{\vec{f} - \vec{g}}_\infty.
	\end{align*}
\end{proof}

\begin{remark}
	La desigualdad 
	\[\norm{A(s)(\vec{f}(s) - \vec{g}(s))}_2 
		\leq \norm{A}_\infty \norm{\vec{f} - \vec{g}}_\infty\] 
	no es más que la propiedad de Lipschitz para \(A\), 
	no hace falta la linealidad.
\end{remark}

\begin{corollary} \label{cor:T_norm_infty_ineq}
	Se da la desigualdad
	\[\norm{T^m(\vec{f}) - T^m(\vec{g})}_\infty 
		\leq \frac{\norm{A}^m_\infty}{m!} (\tilde{\omega} - \tilde{\alpha})^m
		\norm{\vec{f} - \vec{g}}_\infty.\]
\end{corollary}

\begin{proof}
	Tomamos supremos en \(t\) en el lema~\ref{lem:desinfty}.
\end{proof}

Después de todos estos preámbulos podemos finalmente demostrar el teorema de
Picard.

En particular, existe \(m \in \N\) tal que \(T^m\) es contractiva, y queda así
demostrado el teorema de Picard:

\begin{theorem}[Picard]
	Si \(A(t)\), \(\vec{b}(t)\) son continuas en 
	\((\alpha, \omega) \subset \R\), \(t_0 \in \R\) y \(\vec{x}^0 \in \R^n\), 
	entonces el problema del valor inicial:
	\[\eqsys{
		\vec{x}' = A(t)\vec{x} + \vec{b}(t) \\
		\vec{x}(t_0) = \vec{x}^0}\]
	tiene solución única en \((\alpha, \omega)\).
\end{theorem}

\begin{proof}
	Por el corolario~\ref{cor:T_norm_infty_ineq} existe un \(m \in \N\) tal que 
	\(T^m\) es contractiva. Podemos entonces aplicar el 
	corolario~\ref{cor:banach_fixed_point} con lo que \(T\) tiene un único 
	punto fijo. Ahora bien, puesto que \(\vec{x}\) es solución del
	PVI si y solo si \(T\vec{x} = \vec{x}\) obtenemos el resultado.
\end{proof}

\begin{remark}
	La misma demostración prueba existencia y unicidad para el caso no lineal bajo
	hipótesis de Lipschitzianidad en el \textquote{operador diferencial}.
\end{remark}

Observamos que \(T^m(\vec{f}_0)\), donde \(\vec{f}_0\) es un punto inicial 
cualquiera, converge exponencialmente rápido al punto fijo de \(T\). 
Esto nos da un método numérico para aproximar la solución del PVI escogiendo un 
punto inicial simple (constante). También nos proporciona el desarrollo en serie
(analítico) de la solución del PVI, este método se conoce como método de las 
iteraciones de Picard.

\begin{corollary}[Iteraciones de Picard]
	La solución al anterior problema del valor inicial es el límite de la 
	sucesión:
	\[\eqsys{
		\vec{x}_0 = T(\vec{f}_0) & \vec{f}_0 : (\alpha, \omega) \to \R^n \\
		\vec{x}_m = T(\vec{x}_{m - 1})}\]
	Si la solución es analítica la anterior sucesión proporciona su desarrollo
	en serie.
\end{corollary}

\begin{example}
	Resolver el siguiente PVI mediante las iteraciones de Picard,
	\[\eqsys{x' = ax \quad \text{(ecuación escalar)} \\
			x(0) = x_0}\]
\end{example}

\begin{solution}
	Iniciamos el proceso con la solución constante \(x_0\),
	\[x_1 = T(x_0) = x_0 + \int_0^t a x_0 \dif s = x_0 + x_0 a t,\]
	reiterando tenemos que
	\[x_n = T(x_{n - 1}) = x_0 \sum_{k = 0}^n \frac{(at)^k}{k!}.\]
	Cuando \(n\) tiende a infinito nos queda 
	\(x_\infty := \lim_n x_n = x_0 e^{at}\).
\end{solution}

\begin{remark}
	Este procedimiento también vale, con las modificaciones obvias, para el caso
	matricial, a diferencia de la separación de variables que ya vimos.
\end{remark}

\section{Estructura del espacio de soluciones}

\subsection{Caso homogéneo}

Consideramos el sistema homogéneo \(\vec{x}' = A(t) \vec{x}\) y definimos el
operador 
\begin{align*}
	T : C^1((\alpha, \omega), \R^n) & \to C((\alpha, \omega), \R^n) \\
							\vec{x} & \mapsto \vec{x}' - A(t)\vec{x},
\end{align*}
claramente \(\vec{x}\) es solución de la ecuación si y solo si 
\(\vec{x} \in \ker(T)\).
Puesto que \(T\) es lineal el conjunto de soluciones es un espacio vectorial,
más propiamente un subespacio vectorial de \(C^1((\alpha, \omega), \R^n)\).

\begin{theorem}
	El espacio de soluciones de un sistema de ecuaciones lineales homogéneas es
	un subespacio vectorial de \(C^1((\alpha, \omega), \R^n)\) de dimensión
	\(n\). Además, si \(\vec{\phi}_j\) es la solución del problema de valor 
	inicial
	\[\eqsys{
		\vec{x}' = A(t)\vec{x} \\
		\vec{x}(t_0) = \vec{e}_j = (0, \dots, \underset{j}{1}, \dots, 0)}\]
	siendo \(t_0 \in (\alpha, \omega)\) arbitrario, tenemos que
	\(\set{\vec{\phi}_1, \dots, \vec{\phi}_n}\) es base del susodicho 
	espacio de soluciones.
\end{theorem}

\begin{proof}
	Demostramos primero que los \(\vec{\phi}_j\) son linealmente independientes. 
	Si \(\alpha_1, \dots, \alpha_n\) son tales que 
	\(\sum_{j = 1}^n \alpha_j \vec{\phi}_j \equiv \vec{0}\),
	evaluando en \(t_0\) se obtiene
	\[\sum_{j = 1}^n \alpha_j \vec{e}_j = \vec{0} \implies \alpha_j = 0,\]
	para todo \(j = 1, \dots, n\) como queríamos, por ser 
	\(\vec{e}_1, \dots, \vec{e}_n\) base. 
	Demostramos ahora que es sistema de generadores. Sea \(\vec{y}\) solución
	de la ecuación lineal y sea 
	\(\vec{y}(t_0) = (\alpha_1, \dots, \alpha_n)\),
	definimos \(\vec{z} = \sum_{j = 1}^n \alpha_j \vec{\phi}_j\) y observamos 
	que tanto \(\vec{y}\) como \(\vec{z}\) son soluciones de
	\[\eqsys{
		\vec{x}' = A(t)\vec{x} \\
		\vec{x}(t_0) = (\alpha_1, \dots, \alpha_n)}\]
	de donde deducimos que \(\vec{y} = \vec{z}\) en virtud del 
	Teorema de Picard.
\end{proof}

\begin{definition}[Matriz fundamental]
	Llamaremos matriz fundamental del sistema 
	\(\vec{x}' = A(t)\vec{x}\) y la denotaremos
	\(\Phi(t)\) a cualquier matriz cuyas columnas formen base del espacio de
	soluciones del mismo.
\end{definition}

Con las definiciones anteriores tenemos que
\(\Phi(t) := (\vec{\phi}_1, \vec{\phi}_2, \dots, \vec{\phi}_n)\) es una matriz 
fundamental. Por la propia definición tenemos que si \(\Phi\) es una matriz 
fundamental de un sistema de ecuaciones lineales homogéneas entonces la solución 
general del sistema es \(\Phi(t)\vec{c}\) con \(\vec{c} \in \R^n\):
\[\vec{x}(t) = \Phi(t) \vec{c} 
	= (\vec{\phi}_1, \dots, \vec{\phi}_n) \mat{c_1 \\ \vdots \\ c_n} =
	c_1\phi_1 + \cdots + c_n\phi_n.\]

\begin{corollary}
	Sea \(\Phi(t)\) una matriz cuyas columnas son solución del sistema
	\(\vec{x}' = A(t)\vec{x}\). Las siguientes proposiciones son equivalentes:
	\begin{multicols}{2}
	\begin{enumerate}[i)]
		\item \(\Phi(t)\) es matriz fundamental.

		\item \(\det(\Phi(t)) \neq 0 \ \forall t \in (\alpha, \omega)\).

		\item \(\exists t_0 \in (\alpha, \omega) \ \det(\Phi(t_0)) \neq 0\).
	\end{enumerate}
	\end{multicols}
\end{corollary}

Las columnas de \(\Phi(t)\) son soluciones de \(\vec{x}' = A(t)\vec{x}\) si y 
solo si \(\Phi(t)\) es solución de la ecuación matricial \(X' = A(t)X\), donde
\begin{align*}
	X : (\alpha, \omega) & \to \mathcal{M}_{n \times n} \\
	t                    & \mapsto X(t)
\end{align*}
Asimismo, que \(\Phi(t)\) sea matriz fundamental equivale a que sea solución de
un problema de valor inicial de la forma:
\[\eqsys{X' = A(t)X \\ X(t_0) = X^0}\]
donde ambas ecuaciones son matriciales y \(\det(X_0) \neq 0\). Notamos que la
matriz fundamental \(\Phi(t) = (\vec{\phi}_1, \dots, \vec{\phi}_n)\) que 
definimos anteriormente es la única solución del problema de valor inicial:
\[\eqsys{X' = A(t)X \\ X(0) = \mathit{Id}}\]

\subsection{Caso no homogéneo}

La misma demostración del caso escalar prueba que la solución general de
\(\vec{x}' = A(t)\vec{x} + \vec{b}(t)\) es \(\vec{x}_p(t) + \vec{x}_h(t)\), 
donde \(\vec{x}_p(t)\) es una solución particular arbitraria del sistema y 
\(\vec{x}_h(t)\) es la solución general del sistema homogéneo asociado 
\(\vec{x}' = A(t)\vec{x}\). Por lo visto arriba,
\(\vec{x}_h(t) = \Phi(t)\vec{c}\) con \(\vec{c} \in \R^n\), y como veremos más 
adelante existen diversos métodos para obtener \(\vec{x}_p(t)\).

\section{Método de variación de constantes}

Un método para obtener una solución particular \(\vec{x}_p(t)\) de un sistema
\(\vec{x}' = A(t)\vec{x} + \vec{b}(t)\) es la variación de constantes, que como
veremos es muy similar al al caso de una variable. 

Sabemos que la solución general de la ecuación homogénea asociada es 
\(\vec{x}_h(t) = \Phi(t)\vec{c}\), conjeturamos la existencia
de \(\vec{x}_p(t) = \Phi(t)\vec{c}(t)\):
\[\vec{x}'_p(t) 
	= \Phi'(t) \vec{c}(t) + \Phi(t) \vec{c}'(t) 
	= A(t)\Phi(t) \vec{c}(t) + \Phi(t) \vec{c}'(t).\]
Además, puesto que es solución del sistema homogéneo
\[\vec{x}'_p(t) = A(t)\vec{x}_p(t) + \vec{b}(t) 
	= [A(t) \Phi(t) \vec{c}(t)] + \vec{b}(t),\]
igualando ambas expresiones
\[\vec{c}'(t) = \Phi^{-1}(t) \vec{b}(t).\]

Basta por tanto elegir 
\[\vec{c}(t) = \int_{t_0}^t \Phi^{-1}(s) \vec{b}(s) \dif s.\]

Hemos demostrado el siguiente teorema:

\begin{theorem}
	Sea \(\Phi(t)\) una matriz fundamental del sistema 
	\(\vec{x}' = A(t)\vec{x}\), entonces
	\[\vec{x}_p(t) = \Phi(t) \int_{t_0}^t \Phi^{-1}(s) \vec{b}(s) \dif s\]
	con \(t_0 \in (\alpha, \omega)\) es una solución particular de
	\(\vec{x}' = A(t)\vec{x} + \vec{b}(t)\) que satisface 
	\(\vec{x}_p(t_0) = \vec{0} \in \R^n\).
\end{theorem}

\end{document}
