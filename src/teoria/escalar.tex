\documentclass[../ecuaciones_diferenciales.tex]{subfiles}

\begin{document}

\begin{definition}
	\label{def:sisescordsup}
	Una ecuación diferencial escalar de orden superior es una ecuación de la
	forma:
	\[x^{(n)} = a_n(t) x^{(n - 1)} + \cdots + a_1(t)x + a_0(t).\]
\end{definition}

Definiendo las variables \(x_n = x^{(n - 1)}\), o lo que es lo mismo,
\(x_n = x'_{n - 1}\), el sistema~\ref{def:sisescordsup} es equivalente a:
\[\eqsys{
	x_2 &= x'_1 \\
	x_3 &= x'_2 \\
	&\vdots \\
	x_n &= x'_{n - 1} \\
	x'_n &= a_n(t) x_n + \cdots + a_1(t) x_1 + a_0(t)
	}\]

donde la última ecuación es de primer orden, podemos también expresar este
sistema matricialmente como \(x' = A(t)x + b(t)\) con:
\[A(t) = \mat{
		0 & 1 & & \\
		& \ddots & \ddots & \\
		& & 0 & 1 \\
		a_1(t) & a_2(t) & \cdots & a_n(t)
	}
	\quad \text{y} \quad
	b(t) = \mat{0 \\ \vdots \\ 0 \\ a_0(t)}.
\]

En el caso del sistema el problema del valor inicial consiste en fijar \(x_1(t_0),
x_2(t_0), \dots, x_n(t_0)\), lo que equivale aquí a fijar \(x(t_0), x'(t_0),
\dots, x^{(n-1)}(t_0)\). Aplicando lo visto anteriormente el sistema tiene
solución general dada por \(x_p(t) + x_h(t)\) donde, como de costumbre,
\(x_p(t)\) es una solución particular y \(x_h(t)\) es la solución general de la
ecuación homogénea asociada, la cual es un espacio vectorial de dimensión \(n\).

Si \(\phi_1, \dots, \phi_n\) son soluciones linealmente independientes
de~\ref{def:sisescordsup} la matriz fundamental asociada al sistema equivalente
es
\[\Phi(t) = \mat{
		\phi_1 & \dots & \phi_n \\
		\phi'_1 & \dots & \phi'_n \\
		\vdots & & \vdots \\
		\phi^{(n - 1)}_1 & \dots & \phi^{(n - 1)}_n \\
	}.\]

\begin{definition}
	Llamaremos Wronskiano de \(\phi_1, \dots, \phi_n\) al determinante
	\[W(\phi_1, \dots, \phi_n)(t) = \det\mat{
			\phi_1(t) & \dots & \phi_n(t) \\
			\phi'_1(t) & \dots & \phi'_n(t) \\
			\vdots & & \vdots \\
			\phi^{(n - 1)}_1(t) & \dots & \phi^{(n - 1)}_n(t) \\
		}, \quad t \in (\alpha, \omega).\]
\end{definition}

\section{Matriz fundamental}

Hemos dejado pendiente cómo obtener una matriz fundamental \(\Phi(t)\) asociada
al sistema homogéneo \(x' = A(t)x\), por ahora nos restringiremos al caso de
coeficientes constantes, es decir, \(x' = Ax\) donde \(A\) es una matriz
constante, no depende de \(t\) y nos centraremos en particular en funciones de
dominio \(\R\).

En el caso escalar, \(x' = ax\), sabemos que la solución es \(x = ke^{at}\),
donde podemos interpretar \(e^{at}\) como una \textquote{matriz fundamental
	\(1 \times 1\)}. En el caso matricial veremos que una matriz fundamental es
\(\Phi(t) = e^{At}\), aunque primero tendremos que definir la exponencial de una
matriz y obtener métodos para computarla.

\begin{lemma}
	\label{lem:expphi}
	Sea \(\Phi(t)\) la matriz fundamental asociada a las soluciones de
	\[\eqsys{
			x' = Ax \\
			x(0) = e_j
		}\]
	o, equivalentemente, la única solución del problema de valor inicial
	\[\eqsys{
			X' = AX \\
			X(0) = \mathit{Id}
		}\]
	Se dan las siguientes igualdades:
	\begin{multicols}{3}
		\begin{enumerate}[i)]
			\item \(\displaystyle \Phi(0) = \mathit{Id}\)
			\item \(\displaystyle \Phi(t + s) = \Phi(t) \Phi(s)\)
			\item \(\displaystyle \Phi(-t) = \Phi^{-1}(t)\)
		\end{enumerate}
	\end{multicols}
\end{lemma}

\begin{proof}
	\begin{enumerate}[i), wide, labelwidth=0pt, labelindent=0pt]
		\item La primera igualdad se sigue directamente de la definición.
		\item Fijamos \(s\) y consideramos el problema de valor inicial
		      \[\eqsys{ x' = Ax \\ x(0) = \Phi(s) }\]
		      Vamos a ver que tanto \(\Phi(t + s)\) como \(\Phi(t)\Phi(s)\) (como
		      funciones en \(t\)) son soluciones de ese problema. Consideramos
		      primero \(X(t) := \Phi(t + s)\), tenemos que \(X(0) = \Phi(s)\) y
		      \[X'(t) = \Phi'(t + s) = A \Phi(t + s) = A X(t),\]
		      similarmente para \(Y(t) := \Phi(t)\Phi(s)\), aplicando la primera
		      igualdad \(Y(0) = \Phi(0)\Phi(s) = \Phi(s)\) y
		      \[Y'(t) = \Phi'(t)\Phi(s) = A \Phi(t)\Phi(s) = A Y(t).\]
		\item Aplicando la primera y segunda igualdad en los respectivos
		      miembros:
		      \[\mathit{Id} = \Phi(t + (-t)) = \Phi(t)\Phi(-t).\]
	\end{enumerate}
\end{proof}

Para la matriz fundamental \(\Phi\) así definida, \(x(t) = \Phi(t)x^0\)
es la solución del sistema
\[\eqsys{x' = Ax \\ x(0) = x^0}\]

\section{Exponencial de una matriz}

Notamos que la matriz que acabamos de definir \(\Phi\) tiene muchas de las
propiedades de la función exponencial, de hecho más adelante demostraremos que
\(\Phi(t) = \sum_{k=0}^\infty (tA)^k/k!\), por ahora nos contentamos con la
siguiente definición:

\begin{definition}
	Dada una matriz \(B\) se define \(\exp(B)\), o con la notación habitual
	\(e^B\), como
	\[e^B = \sum_{k = 0}^\infty \frac{B^k}{k!}.\]
\end{definition}

Presentamos ahora algunas propiedades típicas de la función exponencial que
también cumple esta versión matricial, la mayoría son corolario de lo visto
arriba.

\begin{proposition}
	Se cumplen las siguientes igualdades.
	\begin{multicols}{2}
		\begin{enumerate}[i)]
			\item \(\displaystyle e^0 = \mathit{Id}\)
			\item \(\displaystyle e^{(t + s)A} = e^{tA} e^{sA}\)
			\item \(\displaystyle \parens{e^{tA}}^{-1} = e^{-tA}\)
			\item \(\displaystyle \frac{d}{\dif t} e^{tA} = A e^{tA}\)
		\end{enumerate}
	\end{multicols}
\end{proposition}

\begin{remark}
	No es cierto en general que \(e^{A + B} = e^A e^B\), solo se cumple si las
	matrices \(A\) y \(B\) conmutan.
\end{remark}

\begin{theorem}
	La matriz \(\Phi(t)\) definida en~\ref{lem:expphi} es igual a \(e^{At}\),
	dicho de otro modo:
	\[\Phi(t) = \sum_{k = 0}^\infty \frac{t^k A^k}{k!} =
		\sum_{k = 0}^\infty \frac{(tA)^k}{k!}.\]
\end{theorem}

\begin{proof}
	Haremos uso de las iteraciones de Picard, para ello definimos
	\begin{align*}
		T : C((\alpha, \omega), \mathcal{M}_{n \times n}) & \to
		C((\alpha, \omega), \mathcal{M}_{n \times n})                                                        \\
		\Psi                                              & \mapsto \mathit{Id} + \int_0^t A \Psi(s) \dif s.
	\end{align*}

	Comenzamos la iteración con \(X_0(t) = \mathit{Id}\) y proseguimos:
	\begin{align*}
		X_1 & = T(X_0) = \mathit{Id} + \int_0^t A \mathit{Id} \dif s =
		\mathit{Id} + At                                                      \\
		X_2 & = T(X_1) = \mathit{Id} + \int_0^t A (\mathit{Id} + As) \dif s =
		\mathit{Id} + At + \frac{A^2 t^2}{2}                                  \\
		    & \vdots                                                          \\
		X_n & = T(X_{n - 1}) =
		\mathit{Id} + At + \frac{A^2 t^2}{2} + \cdots + \frac{A^n t^n}{n!}
		= \sum_{k = 0}^n \frac{A^k t^k}{k!},
	\end{align*}
	tomando el límite \(\lim_{n \to \infty} X_n\) obtenemos el resultado.
\end{proof}

\section{Métodos de cómputo}

Veamos ahora como podemos computar \(e^{At}\) para una matriz \(A\) arbitraria.
Comenzamos introduciendo métodos para matrices diagonales y diagonalizables y
después para matrices en forma canónica de Jordan. Utilizaremos diversos
resultados propios del álgebra lineal por el camino.

\subsection{Matrices diagonales}

Sea \(A\) una matriz diagonal, entonces:
\[A = \mat{
		\lambda_1 & & \\
		& \ddots & \\
		& & \lambda_n
	}
	\quad \text{y} \quad
	x' = Ax \iff
	\eqsys{
		x'_1 &= \lambda_1 x_1 \\
		&\vdots \\
		x'_n &= \lambda_n x_n \\
	}
\]
La solución general de este sistema viene dada por
\(x_1(t) = k_1 e^{\lambda_1 t}, \dots, x_n(t) = k_n e^{\lambda_n t}\) o, lo que es
lo mismo,
\[x = \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}
	\mat{k_1 \\ \vdots \\ k_n}.\]

Puesto que es solución del problema de valor inicial
\(\set{X' = AX,\ X(0) = \mathit{Id}}\) hemos comprobado a posteriori, por
la unicidad de la solución, que la matriz de la izquierda es \(e^{At}\). Para
convencernos lo comprobamos también directamente, tenemos que
\[e^{At} = \exp\smat{\lambda_1t & & \\ & \ddots & \\ & & \lambda_nt}
	= \sum_{k = 0}^\infty
	\frac{1}{k!}\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n}^k t^k
	= \sum_{k = 0}^\infty
	\frac{1}{k!} \smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k,
\]
ahora, explotando la diagonalidad
\[\sum_{k = 0}^\infty
	\frac{1}{k!} \smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k
	= \sum_{k = 0}^\infty
	\frac{1}{k!} \smat{(\lambda_1 t)^k & & \\ & \ddots & \\ & & (\lambda_n t)^k}
	=
	\mat{\sum_{k = 0}^\infty \frac{(\lambda_1 t)^k}{k!} & & \\
		& \ddots &
		\\ & & \sum_{k = 0}^\infty \frac{(\lambda_n t)^k}{k!}}.
\]
Hemos demostrado así de dos formas distintas el siguiente teorema:

\begin{proposition}
	Sea \(A\) una matriz diagonal, podemos expresar \(e^{At}\) como:
	\[e^{At} = \exp\smat{\lambda_1t & & \\ & \ddots & \\ & & \lambda_nt}
		= \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}.\]
\end{proposition}

\subsection{Matrices diagonalizables con autovalores reales}

Sea \(A\) una matriz diagonalizable con autovalores reales
\(\lambda_1, \dots, \lambda_n\). Si \(\{v_1, \dots, v_n\}\) es una base de
autovectores asociados, entonces
\[
	P^{-1} A P = D = \mat{\lambda_1 && \\ & \ddots & \\ && \lambda_n}
	\quad \text{siendo} \quad
	P = (v_1, \dots, v_n).
\]

Con esto tenemos que la ecuación \(x' = Ax\) se puede escribir
\(x' = P D P^{-1} x\); con el cambio de variable \(y = P^{-1}x\), nos queda
\[x' = Ax \iff x' = PDP^{-1}x \iff P^{-1}x' = DP^{-1}x \iff y' = Dy,\]
donde es fundamental que \(P\) (y por tanto \(P^{-1}\)) sea una matriz constante
para poder afirmar \(y' = (P^{-1}x)' = P^{-1}x'\). Con este cambio de variable
podemos expresar la ecuación como
\[y' = \mat{\lambda_1 && \\ & \ddots & \\ && \lambda_n} y \iff
	y = e^{Dt}k \iff y = \mat{k_1 e^{\lambda_1 t} \\ \vdots \\
		k_n e^{\lambda_n t}},\]
con \(k = (k_1, \dots, k_n)^t \in \R^n\).
Tras estos preámbulos podemos obtener
una expresión explícita del espacio de soluciones
\begin{align*}
	x &= Py = (v_1, \dots, v_n) \mat{k_1 e^{\lambda_1 t} \\ \vdots \\ k_n e^{\lambda_n t}} =
    k_1 e^{\lambda_1 t} v_1 + \cdots + k_n e^{\lambda_n t}v_n \\
	  &= P \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}
      \mat{k_1 \\ \vdots \\ k_n} = P e^{D t} P^{-1} \mat{c_1 \\ \vdots \\ c_n} =
      e^{A t} \mat{c_1 \\ \vdots \\ c_n}
\end{align*}

\begin{remark}
  El espacio de soluciones es el espacio vectorial generado por
  \[e^{\lambda_1 t} v_1, \dots, e^{\lambda_n t} v_n.\]
\end{remark}
\pagebreak

\subsection{Matrices diagonalizables con autovalores complejos}

Consideramos ahora el caso diagonalizable general (\(A\) puede tener
autovalores complejos).

\begin{proposition}
	Si \(A\) tiene coeficientes reales y \(\lambda\) es autovalor de \(A\),
	entonces su conjugado \(\bar{\lambda}\) también lo es.
	\begin{proof}
		El polinomio característico de \(A\), \(p(\lambda) = \det(A-\lambda I)\)
		tiene coeficientes reales, es decir, \(p(\lambda) = \sum_{i=1}^n
		a_i\lambda^i\) con \(a_i \in \R\).

		Si \(\lambda_0 \in \Complex\) es raíz del polinomio, es decir, \(\sum_{i=1}^n
		a_i\lambda_0^i = 0\), entonces

		\[0 = \bar{0} = \overline{\sum_{i=1}^n a_i\lambda_0^i} = \sum_{i=1}^n
			\overline{a_i}(\overline{\lambda_0})^i = \sum_{i=1}^n
			a_i(\overline{\lambda_0})^i = p(\overline{\lambda_0})\]
	\end{proof}
\end{proposition}

\begin{remark}
	Esto quiere decir que los autovalores no reales siempre vienen por pares,
	por lo que nos restringiremos de momento a subespacios de dimensión 2.
\end{remark}

\begin{remark}
	En estas condiciones, ya sabemos resolver \(x' = Ax\) en \(\Complex\), es
	decir, si permitimos como solución \(z(t) = x(t) + iy(t)\) porque todo lo
	que hemos visto funciona igual para números complejos. Vamos a ver cómo
	aprovechar esto para transformar una solución con números complejos en otra
	equivalente que sólo involucre números reales.
\end{remark}

\begin{proposition}
	Si \(\lambda = a + ib\) es un autovalor complejo de \(A\) y \(w = u + iv\)
	un autovector asociado, entonces \(\bar{\lambda} = a - ib\) es también
	autovalor y \(\bar{w} = u - iv\) es un autovector asociado.
	\begin{proof}
		Como \(\lambda\) es un autovalor con autovector \(w\), se cumple
		\(Aw = \lambda w\), por lo que
		\(\overline{Aw} = \overline{\lambda w} = \bar{\lambda} \bar{w}\); por otro
		lado, \(\overline{Aw} = \bar{A} \bar{w} = A \bar{w}\), luego \(A \bar{w} =
		\bar{\lambda} \bar{w}\).
	\end{proof}
\end{proposition}

Sabemos, por lo visto anteriormente, que \(z(t) = e^{\lambda t} w\) y
\(\bar{z}(t) = e^{\bar{\lambda} t} \bar{w}\) son soluciones de \(x' = Ax\) y
son linealmente independientes. Sin más que desarrollar los productos, se
llega a:
\begin{gather*}
	z(t) = e^{(a+ib)t} (u+iv) = e^{at}\left( (u\cos bt - v\sin bt) + i(v\cos bt + u\sin bt) \right) \\
	\bar{z}(t) = e^{(a-ib)t} (u-iv) = e^{at}\left( (u\cos bt - v\sin bt) - i(v\cos bt + u\sin bt) \right)
\end{gather*}

Como
\[\mat{\Re z(t) \\ \Im z(t)} = \frac{1}{2} \mat{z(t) + \bar{z}(t) \\
		(z(t) - \bar{z}(t))/i} = \frac{1}{2}\mat{1 & 1 \\ -i & i}\mat{z(t) \\
		\bar{z}(t)} \]

y \(\det\smat{1 & 1 \\ -i & i} \neq 0\), se cumple que \(\Re z(t)\) y
\(\Im z(t)\) siguen siendo soluciones y linealmente independientes, que además
generan el mismo espacio de soluciones. Tenemos entonces las soluciones reales
\[
	\begin{cases}
		x(t) = \Re z(t) = e^{at}(u\cos bt - v\sin bt) \\
		y(t) = \Im z(t) = e^{at}(v\cos bt + u\sin bt)
	\end{cases}
\]
Consideramos \(A \in \mathcal{M}_{2 \times 2}(\R)\) con autovalores
\(\lambda = a+ib, \bar{\lambda} = a-ib \in \Complex\), con autovectores
asociados \(w = u+iv, \bar{w} = u-iv\). Se tiene
\[Au + iAv = Aw = \lambda w = (a+ib)(u+iv) = (au-bv) + i(av+bu),\]
de donde, igualando partes real e imaginaria,
\[
	\begin{cases}
		Au = au-bv \\
		Av = bu+av
	\end{cases}
\]
Ya hemos visto que \(u\) y \(v\) son linealmente independientes, lo que nos
permite definir una matriz de paso \(P = \mat{u & v}\). Entonces,
\[\mat{Au & Av} = A \mat{u & v} = \mat{au-bv & bu+av} = \mat{u & v} \mat{a & b
		\\ -b & a} \implies P^{-1}AP = \mat{a & b \\ -b & a}\]
Nos centramos ahora en sistemas de la forma \(x' = \smat{a & b \\ -b & a}x\),
que tienen por solución general \(x(t) = \exp \smat{at & bt \\ -bt & at}
\smat{c_1 \\ c_2}\), \(\smat{c_1 \\ c_2} \in \R^2\). Nos falta ahora calcular
esta exponencial.
\begin{align*}
	\exp\mat{at & bt                            \\ -bt & at} &= \exp\left(a\mat{1 & 0 \\ 0 & 1}t + b\mat{0 & 1 \\
	-1          & 0}t\right) = \mat{e^{at}  & 0 \\ 0 & e^{at}} \exp\left(b\mat{0 & 1
	\\ -1 & 0}t\right) \\
	            & = e^{at}\exp\left(b\mat{0 & 1 \\ -1 & 0}t\right),
\end{align*}

por lo que nos basta analizar el caso \(y' = \smat{0 & 1 \\ -1 & 0}y\), es
decir, calcular \(\exp \smat{0 & t \\ -t & 0}\). Para ello, diagonalizamos la
matriz \(M := \smat{0 & 1 \\ -1 & 0}\):

Su polinomio característico es \(p(\lambda) = \lambda^2 + 1\), luego sus
autovalores son \(\pm i\). Los espacios invariantes son, respectivamente,
\[\ker(M - iI) = L\left[\mat{1 \\ i}\right] \quad \text{y} \quad \ker(M + iI) =
	L\left[\mat{i \\ 1}\right],\]

con lo que una posible matriz de paso es \(P := \smat{1 & i \\ i & 1}\), con
inversa \(P^{-1} = \frac{1}{2} \smat{1 & -i \\ -i & 1}\). Se tiene entonces
\begin{gather*}
	\mat{0 & 1 \\ -1 & 0} = \mat{1 & i \\ i & 1} \mat{i & 0 \\ 0 & -i}
	\mat{1 & i \\ i & 1} ^{-1} \implies \\
	\exp \mat{0 & t \\ -t & 0} = \frac{1}{2} \mat{1 & i \\ i & 1}
	\mat{e^{it} & 0 \\ 0 & e^{-it}} \mat {1 & -i \\ -i & 1} =
	\frac{1}{2} \mat{e^{it}+e^{-it} & -ie^{it}+ie^{-it} \\ ie^{it}-ie^{-it} &
	e^{it}+e^{-it}} \implies \\
	\exp \mat{0 & t \\ -t & 0} = \mat{\cos t & \sin t \\ -\sin t & \cos t}
\end{gather*}

Volviendo al sistema que queríamos resolver, es decir, \(x' = \smat{a & b \\
	-b & a}x\), se tiene

\[A = P \mat{a & b \\ -b & a} P^{-1} \implies e^{At} = P \mat{e^{at}\cos bt &
	e^{at}\sin bt \\ -e^{at} \sin bt & e^{at} \cos bt} P^{-1}\]

y, así, la solución general del problema es

\[x(t) = P \mat{e^{at}\cos bt &
	e^{at}\sin bt \\ -e^{at} \sin bt & e^{at} \cos bt} P^{-1} \mat{k_1 \\
		k_2} = P \mat{e^{at}\cos bt & e^{at}\sin bt \\ -e^{at} \sin bt & e^{at}
	\cos bt} \mat {c_1 \\ c_2}\]

En el caso diagonalizable más general, es decir, con autovalores reales y
complejos, basta combinar todo lo que hemos visto: si \(\lambda_1, \dots,
\lambda_r\) son los autovalores reales de \(A\) y \(a_1 \pm ib_1, \dots, a_s
\pm ib_s (b_j>0)\) son sus autovalores complejos (en ambos casos se admiten
repeticiones), se considera la matriz de paso
\[P = \mat{v_1 & \cdots & v_r & \Re w_1 & \Im w_1 & \cdots & \Re w_s & \Im w_s}\]
siendo \(v_j\) y \(w_j\) autovectores asociados a \(\lambda_j\) y \(a_j
+ib_j\), respectivamente, de forma que se tiene la descomposición

\[P^{-1}AP = \mat{\lambda_1 & & & & & & & \\
		& \ddots & & & & & & \\
		& & \lambda_r & & & & & \\
		& & & a_1 & b_1 & & & & \\
		& & & -b_1 & a_1 & & & & \\
		& & & & & \ddots & & \\
		& & & & & & a_s & b_s \\
		& & & & & & -b_s & a_s},\]

así llamando
\(R_k(t) = \mat{e^{a_k t} \cos b_k t & e^{a_k t} \sin b_k t \\
-e^{a_k t} \sin b_k t & e^{a_k t} \cos b_k t }\)
la solución general del sistema es:

\[x(t) = P
	\mat{e^{\lambda_1 t} & & & & & \\
		& \ddots & & & & \\
		& & e^{\lambda_r t} & & & \\
		& & & R_1(t) & & \\
		& & & & \ddots & \\
		& & & & & R_s(t) }
	\mat{c_1 \\ \vdots \\ c_r \\ k_1 \\ k_2 \\ \vdots \\ k_{2s-1}\\ k_{2s}}.\]

\begin{example}
	Resolver el sistema \(x' = Ax\), siendo
	\(A = \smat{1 & 0 & 0 \\ 6 & 2 & -3 \\ 1 & 3 & 2}\).
\end{example}

\begin{solution}
	En primer lugar hay que diagonalizar la matriz: sus autovalores son
	\(\lambda_1 = 1\), \(\lambda_2 = 2+3i\) y \(\lambda_3 = 2-3i\), con espacios
	propios asociados

	\[\ker(A-\lambda_1 I) = L \left[ \mat{10 \\ 3 \\ -19} \right], \quad
		\ker(A-\lambda_2 I) = L \left[ \mat{0 \\ 1 \\ -i} \right], \quad
		\ker(A-\lambda_3 I) = L \left[\mat{0 \\ 1 \\ i} \right],\]

	con lo que podemos tomar

	\[v_1 = \mat{10 \\ 3 \\ -19}, \quad v_2 = \Re \mat{0 \\ 1 \\ -i} = \mat{0
			\\ 1 \\ 0}, \quad v_3 = \Im \mat{0 \\ 1 \\ -i} = \mat{0 \\ 0 \\ -1}.\]

	Así, la matriz de paso \(P = \smat{10 & 0 & 0 \\ 3 & 1 & 0 \\ -19 & 0 &
		-1}\) es tal que

	\[P^{-1}AP = \mat{1 & & \\ & 2 & 3 \\ & -3 & 2},\]

	que no es diagonal pero sí \emph{diagonal por bloques}, lo que nos permite
	tomar la exponencial (por bloques) igual que en el caso diagonal:

	Como \(A = P \smat{1 & & \\ & 2 & 3 \\ & -3 & 2} P^{-1}\), su exponencial es

	\[e^{At} = P \mat{e^t & & \\ & e^{2t} \cos 3t & e^{2t} \sin 3t \\ &
		-e^{2t} \sin 3t & e^{2t} \cos 3t} P^{-1},\]

	con lo que la solución general del sistema es

	\[x(t) = P \mat{e^t & & \\ & e^{2t} \cos 3t & e^{2t} \sin 3t \\ &
		-e^{2t} \sin 3t & e^{2t} \cos 3t} \mat{c_1 \\ c_2 \\ c_3}, \quad
		\mat{c_1 \\ c_2 \\ c_3} \in \R^3.\]
\end{solution}

\subsection{Matrices no diagonalizables}

Para terminar con el análisis de los sistemas de ecuaciones diferenciales
lineales con coeficientes constantes, únicamente nos falta estudiar qué ocurre
cuando la matriz que define el sistema no es diagonalizable.

\begin{theorem}[forma canónica de Jordan compleja]
	Sea \(A \in \mathcal{M}_{n \times n}(\Complex)\) con \(s\) autovectores
	linealmente independientes. Entonces, existe una matriz no singular \(P\)
	tal que \(P^{-1} A P = B\), donde \(B\) es una matriz diagonal por bloques,
	es decir,
	\[B = \mat{B_1 & & & \\ & B_2 & & \\ & & \ddots & \\ & & & B_s},\]
	donde cada \emph{bloque de Jordan} \(B_j\ (j = 1, \dots, s)\) es una matriz de
	la forma
	\[B_j = \mat{\lambda & 1 & & \\ & \lambda & \ddots & \\ & & \ddots & 1 \\ & & &
			\lambda},\]
	siendo \(\lambda\) autovalor de \(A\).
\end{theorem}

\begin{remark}
	Cada autovalor de \(A\) apararece en \(B\) tantas veces como su
	multiplicidad algebraica. El número de bloques asociados a un mismo
	autovalor se corresponde con su multiplicidad geométrica.
\end{remark}

\begin{remark}
	El caso diagonalizable corresponde al caso en que todos los bloques son \(1
	\times 1\).
\end{remark}

Veamos cómo calcular la exponencial de un bloque de Jordan arbitrario:

\[B_j = \mat{\lambda & 1 & & \\ & \lambda & \ddots & \\ & & \ddots & 1 \\ & &
		& \lambda} =
	\underbrace{\mat{\lambda & & & \\ & \lambda & & \\ & & \ddots & \\ & & & \lambda}}_D
	+ \underbrace{\mat{0 & 1 & & \\ & 0 & \ddots & \\ & & \ddots & 1 \\ & & & 0}}_N\]

Ya sabemos calcular \(e^{Dt}\), y hallar \(e^{Nt}\) tampoco es complicado. Sus
potencias sucesivas son

\[
	N^0 = \mat{1 & 0 & 0 & \cdots & 0      \\
		& 1 & 0 & \ddots & \vdots \\
		&   & 1 & \ddots & 0      \\
		&   &   & \ddots & 0      \\
		&   &   &        & 1 }, \quad
	N^1 = \mat{0 & 1 & 0 & \cdots & 0      \\
		& 0 & 1 & \ddots & \vdots \\
		&   & 0 & \ddots & 0      \\
		&   &   & \ddots & 1      \\
		&   &   &        & 0 }, \ \ \dots \ \ ,
	N^{n_j-1} = \mat{0 & 0 & 0 & \cdots & 1      \\
		& 0 & 0 & \ddots & \vdots \\
		&   & 0 & \ddots & 0      \\
		&   &   & \ddots & 0      \\
		&   &   &        & 0 }
\]

y, en particular, \(N\) es nilpotente de orden \(n_j\), por lo que basta
aplicar la definición para obtener

\[e^{Nt} = \sum_{k=0}^\infty \frac{t^k}{k!}N^k = \sum_{k=0}^{n_j-1}
	\frac{t^k}{k!}N^k =
	\mat{1 & t & \frac{t^2}{2!} & \cdots & \frac{t^{n_j-1}}{(n_j-1)!} \\
		& 1 & t              & \ddots & \vdots                    \\
		&   & 1              & \ddots & \frac{t^2}{2!}            \\
		&   &                & \ddots & t                         \\
		&   &                &        & 1 }
\]

Finalmente, como \(D = \lambda I\) y \(N\) conmutan, se tiene

\[e^{B_j t} = e^{Dt + Nt} = e^{Dt} e^{Nt} = (e^{\lambda t} I) e^{Nt} =
	e^{\lambda t}e^{Nt} =
	\mat{e^{\lambda t} & te^{\lambda t} & \frac{t^2}{2!}e^{\lambda t} & \cdots & \frac{t^{n_j-1}}{(n_j-1)!}e^{\lambda t} \\
	& e^{\lambda t} & te^{\lambda t}              & \ddots & \vdots                    \\
	&   & e^{\lambda t}              & \ddots & \frac{t^2}{2!}e^{\lambda t}            \\
	&   &                & \ddots & te^{\lambda t}                         \\
	&   &                &        & e^{\lambda t} }.\]

Esto es suficiente en el caso de que \(A\) sólo tenga autovalores reales. Si
hay alguno complejo, hay que trabajar un poco más:

\begin{theorem}[forma canónica de Jordan real]
	Sea \(A \in \mathcal{M}_{n \times n}(\R)\). Existe una matriz no singular
	\(P\) tal que \(P^{-1}AP = B\), donde \(B\) es una matriz diagonal por
	bloques, cada uno de ellos de la forma:
	\begin{itemize}
		\item \(\smat{\lambda & 1 & & \\ & \lambda & \cdot & \\ & & \cdot & 1 \\ & &
			      & \lambda}\), si \(\lambda\) es un autovalor real de \(A\),
		\item \(\smat{D & I_2 & & \\ & D & \cdot & \\ & & \cdot & I_2 \\ & &
			      & D}\), con \(D = \smat{a & b \\ -b & a}\) e \(I_2 = \smat{1 & 0 \\ 0 & 1}\),
		      si \(a+ib\ (b>0)\) es un autovalor complejo de \(A\).
	\end{itemize}
\end{theorem}

A la vista de este teorema, sólo nos falta saber calcular la exponencial de
matrices de la forma

\[\tilde{D} = \mat{D & I_2 & & \\ & D & \ddots & \\ & & \ddots & I_2 \\ & & & D} =
	\mat{D & & & \\ & D & & \\ & & \ddots & \\ & & & D} +
	\underbrace{\mat{0 & I_2 & & \\ & 0 & \ddots & \\ & & \ddots & I_2 \\ & & &
			0}}_{N}.\]

Observamos que estas dos matrices conmutan:
\[\mat{D & & & \\ & D & & \\ & & \ddots & \\ & & & D} N =
	\mat{0 & D & & \\ & 0 & \ddots & \\ & & \ddots & D \\ & & & 0}
	= N \mat{D & & & \\ & D & & \\ & & \ddots & \\ & & & D},\]

luego la exponencial de su suma es
\[\exp(\tilde{D}) t =
	\exp \mat{Dt & & & \\ & Dt & & \\ & & \ddots & \\ & & & Dt} \cdot \exp(Nt)
	= \mat{e^{at}R & & & \\ & e^{at}R & & \\ & & \ddots & \\ & & & e^{at}R}
	\cdot \exp(Nt),\]

puesto que \(\exp \smat{Dt & & & \\ & Dt & & \\ & & \ddots & \\ & & & Dt} =
\smat{e^{Dt} & & & \\ & e^{Dt} & & \\ & & \ddots & \\ & & & e^{Dt}}\) y
\(e^{Dt} = e^{at} \underbrace{\smat{\cos bt & \sin bt \\ -\sin bt & \cos
		bt}}_{R}\). Por otro lado, igual que antes es fácil ver que
\[e^{Nt} = \sum_{k=0}^{n_j-1} \frac{t^k}{k!}N^k =
	\mat{I_2 & tI_2 & \frac{t^2}{2!}I_2 & \cdots & \frac{t^{n_j-1}}{(n_j-1)!}I_2 \\
		& i_2  & tI_2              & \ddots & \vdots                        \\
		&      & I_2               & \ddots & \frac{t^2}{2!}I_2             \\
		&      &                   & \ddots & tI_2                          \\
		&      &                   &        & I_2 }.\]

Juntándolo todo, llegamos a
\[\exp(\tilde{D}) t =
	\mat{e^{\lambda t}R & te^{\lambda t}R & \frac{t^2}{2!}e^{\lambda t}R & \cdots & \frac{t^{n_j-1}}{(n_j-1)!}e^{\lambda t}R \\
	& e^{\lambda t}R & te^{\lambda t}R              & \ddots & \vdots                    \\
	&   & e^{\lambda t}R              & \ddots & \frac{t^2}{2!}e^{\lambda t}R            \\
	&   &                & \ddots & te^{\lambda t}R                         \\
	&   &                &        & e^{\lambda t}R }
\]

Ahora que ya sabemos resolver cualquier sistema con coeficientes constantes,
centrémonos en cómo calcular la forma canónica de Jordan. La clave será
encontrar propiedades de \(B\) que caractericen su estructura de bloques y que
sean invariantes por semejanza, para poder hallarlas a partir de \(A\).

Por ser \(P\) no singular, se cumple \(\dim \ker (B - \lambda I) = \dim \ker
(P^{-1}(A - \lambda I)P) = \dim \ker (A - \lambda I)\) o, equivalentemente,
\(\ran (A - \lambda I) = \ran (B - \lambda I)\).

Recordando la estructura de \(B\), es fácil ver que
\(\dim \ker (B - \lambda I) = \sum_{i=1}^r \nu_i(\lambda)\), siendo
\(\nu_i(\lambda)\) el número de bloques de tamaño \(i \times i\) del autovalor
\(\lambda\).

Consideramos ahora potencias sucesivas de la matriz \(B - \lambda I\). Al
elevar al cuadrado, la dimensión del núcleo aumenta en una unidad por cada
bloque de tamaño mayor que 1:

\[
	B_j - \lambda I =
	\mat{0 & 1 &   &        &        &   \\
		& 0 & 1 &        &        &   \\
		&   & 0 & 1      &        &   \\
		&   &   & \ddots & \ddots &   \\
		&   &   &        & \ddots & 1 \\
		&   &   &        &        & 0 \\} \qquad
	(B_j - \lambda I)^2 =
	\mat{0 & 0 & 1      &        &        &   \\
		& 0 & 0      & 1      &        &   \\
		&   & \ddots & \ddots & \ddots &   \\
		&   &        & \ddots & \ddots & 1 \\
		&   &        &        & \ddots & 0 \\
		&   &        &        &        & 0 \\},
\]
por lo que
\(\dim \ker (B - \lambda I)^2 = \nu_1(\lambda) + 2 \sum_{i=2}^r
\nu_i(\lambda)\). Análogamente, para las terceras potencias se tiene
\(\dim \ker (B - \lambda I)^3 = \nu_1(\lambda) + 2\nu_2(\lambda) + 3
\sum_{i=3}^r \nu_i(\lambda)\) y, en general, para \(k = 1, \dots, r\):
\begin{align*}
	\dim \ker (B - \lambda I)^k & = \sum_{i=1}^{k-1} i \nu_i(\lambda) + k
	\sum_{i=k}^r \nu_i(\lambda)                                                         \\
	                            & = \nu_1(\lambda) + \cdots + (k-1)\nu_{k-1}(\lambda) +
	k[\nu_k(\lambda) + \cdots + \nu_r(\lambda)]
\end{align*}

\begin{remark}
	\(P(B - \lambda I)^kP^{-1} = (A - \lambda I)^k\) y, como el rango es
	invariante por semejanza, \(\dim \ker (A - \lambda I)^k = \dim \ker (B -
	\lambda I)^k\).
\end{remark}

\begin{remark}
	Los números \(\nu_i(\lambda)\) caracterizan \(B\), salvo permutación de bloques.
\end{remark}

Denotamos \(\delta_i(\lambda) = \dim \ker (A - \lambda I)^i\). De esta forma,
\(r\) es el menor número natural tal que \(\delta_r(\lambda) =
\delta_{r+1}(\lambda)\) y se verifica

\[\left\{
	\begin{array}[ht]{r c l}
		\delta_1(\lambda) & =      & \nu_1(\lambda) + \nu_2(\lambda) + \cdots +
		\nu_r(\lambda)                                                                           \\
		\delta_2(\lambda) & =      & \nu_1(\lambda) + 2[\nu_2(\lambda) + \cdots +
		\nu_r(\lambda)]                                                                          \\
		                  & \vdots &                                                             \\
		\delta_r(\lambda) & =      & \nu_1(\lambda) + 2\nu_2(\lambda) + \cdots + r\nu_r(\lambda)
	\end{array}
	\right.
\]

Restando a cada ecuación la anterior, queda un sistema triangular

\[\left\{
	\begin{array}[ht]{r c c c c c c c c}
		\delta_1(\lambda)                          & =      & \nu_1(\lambda) & + & \nu_2(\lambda) & + & \cdots & + & \nu_r(\lambda) \\
		-\delta_1(\lambda) + \delta_2(\lambda)     & =      &                &   & \nu_2(\lambda) & + & \cdots & + & \nu_r(\lambda) \\
		                                           & \vdots &                                                                       \\
		-\delta_{r-1}(\lambda) + \delta_r(\lambda) & =      &                &   &                &   &        &   & \nu_r(\lambda)
	\end{array}
	\right.
\]

que tiene por solución

\[\left\{
	\begin{array}[ht]{r c l}
		\nu_1(\lambda) & = & 2\delta_1(\lambda) - \delta_2(\lambda)                              \\
		\nu_k(\lambda) & = & -\delta_{k-1}(\lambda) + 2\delta_k(\lambda) - \delta_{k+1}(\lambda)
	\end{array}
	\right.
\]

Pasar de la forma de Jordan compleja a la real se hace como uno podría
esperar:

\[B_\Complex =
	\mat{\ddots &      &      &      &      &         \\
		& a+bi & 1    &      &      &         \\
		& 0    & a+bi &      &      &         \\
		&      &      & a-bi & 1    &         \\
		&      &      & 0    & a-bi &         \\
		&      &      &      &      & \ddots} \leadsto
	B_\R =
	\mat{\ddots &    &   &    &      &         \\
		& a  & b & 1  & 0 &         \\
		& -b & a & 0  & 1 &         \\
		&    &   & a  & b &         \\
		&    &   & -b & a &         \\
		&    &   &    &   & \ddots},
\]

pero esto requiere modificar también la matriz de paso compleja:

\[P_\Complex =
	\mat{\cdots & v_i & v_{i+1} & \cdots} \leadsto
	P_\R =
	\mat{\cdots & \Re v_i & \Im v_i & \cdots}\]

Falta únicamente ver cómo calcular
\(P_\Complex = \mat{v_1 & v_2 & \cdots & v_n}\). Para ello, es suficiente
estudiar cómo afecta \(B\) a los vectores de la base canónica, puesto que
\(A\) y la base \(\{v_i : i = 1, \dots, n\}\) se obtienen con el cambio de
base dado por \(P_\Complex\). Si el bloque \(j\)-ésimo de la matriz \(B\) (de
tamaño \(n_j \times n_j\)) empieza en el índice \(l+1\), se tiene

\[\left\{
	\begin{array}[ht]{r c l}
		Av_{l+1}    & =      & \lambda v_{l+1}                 \\
		Av_{l+2}    & =      & \lambda v_{l+2} + v_{l+1}       \\
		            & \vdots &                                 \\
		A v_{l+n_j} & =      & \lambda v_{l+n_j} + v_{l+n_j-1}
	\end{array} \right. \iff
	\left\{
	\begin{array}[ht]{r c l}
		Av_{l+1} & = & \lambda v_{l+1}                               \\
		Av_{l+k} & = & \lambda v_{l+k} + v_{l+k-1}, \ 1 < k \leq n_j \\
	\end{array}
	\right.\]

Estas ecuaciones (junto con el hecho de que \(P\) es invertible)
caracterizan \(P\).
\end{document}
